{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Obtén el HTML de la página cargada\n",
    "html_content = browser.page_source\n",
    "\n",
    "# Analiza el HTML con Beautiful Soup\n",
    "soup = BeautifulSoup(html_content, \"lxml\")\n",
    "\n",
    "import nltk \n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "porter=SnowballStemmer(\"english\") #elimina prefijos y sufijos sin garantizar que se ana palabra valida\n",
    "lmtzr = WordNetLemmatizer()  #creo que te devuelve todo a la root form, si que garantiza que sea valida.\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lower(text):\n",
    "    \"\"\"\n",
    "    Preprocess text by:\n",
    "       - Converting to lowercase.\n",
    "       - Removing punctuation.\n",
    "       - Tokenizing.\n",
    "       - Removing stopwords.\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: A list of tokens.\n",
    "    \"\"\"\n",
    "    # Lowercase the text\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Remove punctuation (everything except word characters and whitespace)\n",
    "    text_no_punct = re.sub(r'[^\\w\\s]', '', text_lower)\n",
    "\n",
    "    # Tokenize the cleaned text.\n",
    "    tokens = word_tokenize(text_no_punct) #divide el texto en palabras individuales\n",
    "\n",
    "    # Filter out stopwords.\n",
    "    filtered_tokens = [token for token in tokens if token not in STOP_WORDS]\n",
    "\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "\n",
    "def preprocess_stem(text):\n",
    "    \"\"\"\n",
    "    Preprocess text by performing all steps in preprocess_lower() and then applying stemming.\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: A list of stemmed tokens.\n",
    "    \"\"\"\n",
    "    # Get the cleaned lowercased and stopwords-removed tokens.\n",
    "    joined_tokens = preprocess_lower(text)\n",
    "    \n",
    "    tokens=word_tokenize(joined_tokens)\n",
    "\n",
    "    # Initialize the Porter Stemmer.\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    # Stem each token.\n",
    "    stemmed_tokens = [ps.stem(token) for token in tokens]\n",
    "\n",
    "    return \" \".join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the processed results\n",
    "processed_descriptions = []\n",
    "\n",
    "# Loop through the raw descriptions in the `descriptions` list\n",
    "for desc in descriptions:\n",
    "    raw_description = desc[\"Raw Description\"] if isinstance(desc, dict) else desc\n",
    "\n",
    "    if raw_description != \"Descripción no encontrada\":\n",
    "        # Apply lowercasing, punctuation removal, and stopword removal\n",
    "        preprocessed_lower = preprocess_lower(raw_description)\n",
    "\n",
    "        # Apply stemming\n",
    "        preprocessed_stem = preprocess_stem(preprocessed_lower)\n",
    "\n",
    "        # Store the processed results in a dictionary\n",
    "        processed_descriptions.append({\n",
    "            \"Raw Description\": raw_description,\n",
    "            \"Processed (Lowercase, No Stopwords)\": preprocessed_lower,\n",
    "            \"Processed (Stemming)\": preprocessed_stem\n",
    "        })\n",
    "    else:\n",
    "        # Handle missing descriptions\n",
    "        processed_descriptions.append({\n",
    "            \"Raw Description\": raw_description,\n",
    "            \"Processed (Lowercase, No Stopwords)\": \"No processing - No description available\",\n",
    "            \"Processed (Stemming)\": \"No processing - No description available\"\n",
    "        })\n",
    "\n",
    "# Print the processed descriptions\n",
    "print(\"\\nProcessed Descriptions:\")\n",
    "for i, desc in enumerate(processed_descriptions):\n",
    "    print(f\"Hotel {i + 1}:\")\n",
    "    print(f\"  Raw Description: {desc['Raw Description']}\")\n",
    "    print(f\"  Processed (Lowercase, No Stopwords): {desc['Processed (Lowercase, No Stopwords)']}\")\n",
    "    print(f\"  Processed (Stemming): {desc['Processed (Stemming)']}\")\n",
    "\n",
    "# Optionally, save the processed descriptions to a CSV for further analysis\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(processed_descriptions)\n",
    "df.to_csv(\"processed_hotel_descriptions.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"\\nProcessed descriptions saved to 'processed_hotel_descriptions.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
